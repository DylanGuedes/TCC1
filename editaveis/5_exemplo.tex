\chapter[EXEMPLO DE USO]{EXEMPLO DE USO}
\label{chapter:example}

Como forma de ilustração do uso do serviço que desenvolvemos, elaboramos uma
aplicação chamada
Forensic\footnote{\url{https://gitlab.com/DGuedes/forensic}}, que interage
com o Shock via Kafka,
respeitando as definições feitas à cerca da API. A aplicação foi feita em
Elixir, uma linguagem funcional desenhada para construir aplicações
manuteníveis e escaláveis, utilizando o \textit{framework} Phoenix, que
facilita no desenvolvimento de aplicações Web.

O Forensic tem como principais objetivos abstrair o Shock do usuário final,
e consequentemente, as ferramentas de Big Data (como o Spark), ao passo em que
fornece um conjunto de funcionalidades que permitem ao usuário final configurar
a atuação dos \textit{streams}, semelhante a outros serviços existentes,
como o Amazon AWS\footnote{\url{https://aws.amazon.com/kinesis/analytics/}}.
Um usuário que deseje interagir com o Shock e o InterSCity deve
(i) configurar um \textit{stream} novo com os
parâmetros desejados; (ii) criar esse \textit{stream} no Shock; (iii) injetar
esse \textit{stream} no Shock; (iv) e iniciar o processamento do
\textit{stream}. O Forensic então abstrai esses 4 passos, facilitando o uso
das ferramentas e da plataforma. A Figura \ref{fig:forensicparams} apresenta
a página de visualização e edição de um \textit{stream}, onde é possível
conferir os parâmetros configurados para cada estágio do \textit{stream}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figuras/pipeline.png}
    \caption{Página de configuração de um \textit{stream}. Os parâmetros
obrigatórios (\textit{topic e brokers}) foram configurados.}
  \label{fig:forensicparams}
\end{figure}

Com o propósito de aproximar as soluções que desenvolvemos a cenários mais reais,
separamos dois casos de uso para serem resolvidos com uso do Forensic e do
Shock, interagindo com o InterSCity, utilizando como bases alguns coletores de
dados de São
Paulo\footnote{\url{https://github.com/lucaskanashiro/collect_sp_data}}. De
maneira geral, os casos de uso abrangem os conceitos citados anteriormente,
como o \textit{cast} de dados, filtros, dentre outros.

\section{CASO DE USO 1 - REGIÕES COM QUALIDADE DO AR INSATISFATÓRIA}

O primeiro caso que separamos é o de regiões que apresentam qualidade do ar
insatisfatória, utilizando como base os dados da
CETESB\footnote{\url{http://sistemasinter.cetesb.sp.gov.br/Ar/php/ar_resumo_hora.php}}.
A solução desse caso de uso pode ser divida em três etapas:
\begin{enumerate}
    \item Coletar os dados;
    \item Publicar os dados no InterSCity;
    \item Configurar um \textit{stream} que filtre os dados, removendo
        da lista os dados com boa qualidade do ar (pois queremos as regiões
        com qualidade insatisfatória).
\end{enumerate}

O primeiro passo, de coleta dos dados, pode ser resolvido via \textit{crawler},
para extrair e normalizar os dados do \textit{site}. Um \textit{script} que
utiliza o Mechanize já havia sido desenvolvido por contribuidores do
InterSCity\footnote{\url{https://github.com/lucaskanashiro/collect_sp_data/blob/master/air_quality.rb}},
e pôde ser reaproveitado.

O segundo passo, de publicação dos dados no InterSCity, se baseia na
normalização dos dados e das requisições, para que aja uma interação com o
Resource Adaptor. Adaptamos o \textit{script} mencionado no passo anterior
para que passasse a fazer requisições REST ao Resource Adaptor do InterSCity,
possibilitando o registro de recursos e o envio dos dados.

O terceiro passo, de configuração do \textit{stream}, é feito através do Forensic.
Um \textit{stream} deve ser criado na interface, e configurado para uso dos
estágios \textit{Kafka Ingestion}, \textit{Kafka Cast} e \textit{Filter}.
O quarto estágio, de \textit{publish}, é mais flexível, e qualquer
estratégia disponível no Forensic serve para esse caso de uso. O Forensic
apresenta uma página de alertas, que é populada via um \textit{flush}
específico apresentado na Listagem \ref{lst:flush1}. Quando esse
\textit{flush} é acionado, o Shock organiza um \textit{job}
que lê de um repositório específico no formato Parquet, e publica no Forensic
os dados via \textit{websocket}.

\lstinputlisting[float,label={lst:flush1},floatplacement=H,language=elixir,caption={Consumo dos dados via \textit{websocket}
caso o evento seja \textit{new\_report}.}]{editaveis/arquivos/channel.ex}

Após o \textit{stream} ser criado e os estágios selecionados, basta
configurar os parâmetros e transferir as definições para o Shock. A Figura
\ref{fig:case1} apresenta uma tela do Forensic com o \textit{stream} já
configurado, utilizando os valores presentes na Tabela \ref{tab:case1}.

\begin{table}[]
    \centering
    \caption{Parâmetros do \textit{stream} utilizado no primeiro caso de uso.}
    \label{tab:case1}
    \begin{tabular}{|c|c|c|}
        \hline
        \textit{\textbf{Estágio}}                   & \textbf{Parâmetro} & \textbf{Valor}                                         \\ \hline
        \multirow{2}{*}{\textit{Kafka Ingestion}} & \textit{topic}     & \textit{interscity}                                    \\ \cline{2-3} 
                                                  & \textit{brokers}   & \textit{kafka:9092}                                    \\ \hline
                                                  \textit{Kafka Cast}                       & -                  & -                                                      \\ \hline
                                                  \textit{Filter}                           & \textit{query}     & \textit{capability == air\_quality AND value != `boa`} \\ \hline
                                                  \textit{Parquet Publishing}               & \textit{path}      & \textit{/analysis}                                     \\ \hline
    \end{tabular}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figuras/parametros.png}
  \caption{Parâmetros do primeiro caso.}
  \label{fig:case1}
\end{figure}

Por fim, basta seguir os passos de transferência que o Forensic sugere (\textit{Create
Stream, Configure Stream, Inject Pipeline e Start Streaming}), presentes no
topo da Figura \ref{fig:case1}. O Shock então começará o processamento dos
dados, e a próxima vez que o usuário visitar a página de alertas e requisitar
atualização, aparecerão os dados insatisfatórios da qualidade do ar da cidade
de São Paulo.

\section{CASO DE USO 2 - MÉDIA DE BICICLETAS LIVRES NA REGIÃO COM COMBINAÇÃO DE STREAMS}

O segundo caso que separamos visa calcular a média de bicicletas livres em estações
de bicicletas de São Paulo, utilizando como base os dados do
CityBik\footnote{\url{https://citybik.es/}}. Esse caso pode ser visto como uma
versão mais complexa do caso anterior, pois deve ser feito o filtro de
capacidades de bicicletas e o cálculo da média dos valores. Ainda,
como mencionado, o Shock só disponibiliza 4 estágios por
\textit{stream}, e como esses requisitos carecem ao menos 5, devem ser criados
dois \textit{streams}, onde o primeiro deve ingerir dados do InterSCity, e o
segundo deve ingerir os resultados do primeiro.

Utilizando os mesmos passos definidos no caso de uso anterior, para o Passo 1
desenvolvemos um
\textit{script}\footnote{\url{https://github.com/lucaskanashiro/collect_sp_data/blob/interscity_integration/citybik.rb}}
que requisita dados à API do CityBik, via REST. Após a ingestão desses dados,
para o Passo 2, o \textit{script} se comunica com o Resource Adaptor,
publicando os dados no InterSCity.

Para o Passo 3, devem ser configurados dois \textit{streams}. O primeiro deve
conter os estágios \textit{Kafka Ingestion}, \textit{Kafka Cast},
\textit{Filter} e \textit{Parquet Publishing}, de modo a ingerir novos dados
do InterSCity, filtrar dados não desejados e disponibilizar os dados em formato
Parquet. O segundo \textit{stream} deve conter os estágios
\textit{InterSCity Parquet Ingestion}, \textit{Mean} e alguma forma de
publicação. A ligação entre os dois \textit{streams} ocorre quando o primeiro
\textit{stream} publica resultados em um arquivo Parquet, e o segundo
\textit{stream} ingere esses resultados como entrada. O valor dos parâmetros
utilizados no primeiro \textit{stream} estão apresentados na Tabela
\ref{tab:case2-1}, e os parâmetros do segundo \textit{stream} na Tabela
\ref{tab:case2-2}.

\begin{table}[]
    \centering
    \caption{Parâmetros do primeiro \textit{stream}.}
    \label{tab:case2-1}
    \begin{tabular}{|c|c|c|}
        \hline
        \textit{\textbf{Estágio}}                   & \textbf{Parâmetro} & \textbf{Valor}                      \\ \hline
        \multirow{2}{*}{\textit{Kafka Ingestion}} & \textit{topic}     & \textit{interscity}                 \\ \cline{2-3} 
                                                  & \textit{brokers}   & \textit{kafka:9092}                 \\ \hline
                                                  \textit{Kafka Cast}                       & -                  & -                                   \\ \hline
                                                  \textit{Filter}                           & \textit{query}     & \textit{capability == free\_bikes}  \\ \hline
                                                  \textit{Parquet Publishing}               & \textit{path}      & \textit{/interscity-data/freebikes} \\ \hline
    \end{tabular}
\end{table}

\begin{table}[]
\centering
    \caption{Parâmetros do segundo \textit{stream}.}
\label{tab:case2-2}
\begin{tabular}{|c|c|c|}
\hline
\textit{\textbf{Estágio}}    & \textbf{Parâmetro} & \textbf{Valor}                      \\ \hline
\textit{Parquet Ingestion} & \textit{path}      & \textit{/interscity-data/freebikes} \\ \hline
\textit{Mean}              & -                  & -                                   \\ \hline
\textit{Memory Publishing} & \textit{table}     & \textit{avg}                        \\ \hline
\end{tabular}
\end{table}


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figuras/caso2-1.png}
  \caption{Parâmetros do primeiro \textit{stream} do segundo caso.}
  \label{fig:case2}
\end{figure}

Após a configuração dos parâmetros e transferência das configurações para o
Shock, semelhante ao ocorrido no caso de uso anterior, os dois \textit{streams}
começam o processamento dos dados, retornando para o Forensic a média de
bicicletas disponíveis na região.
